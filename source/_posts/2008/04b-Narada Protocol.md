---
title: Narada Protocol
tags:
  - p2p
  - protocol
  - video-streaming
id: '37'
categories: 历史归档
permalink: narada-protocol/
date: 2008-04-02 08:50:50
---


<!-- more -->
IP层的协议是按照最小化功能的原则来设计的，只提供了最大努力（best-effort）单播（Unicast）数据报服务，而剩下的包括流控制，差错控制等一系列的功能，都扔给了终端（End System）去实现。

这种设计理念正是Internet能够长足发展的一个非常重要的技术层面的原因（到底为什么呢？）。Internet的长足发展，又刺激了大批应用的开发，而大批应用的出现，又对Internet的基础功能，有了更高的需求。

那么到底应该在IP层提供什么样的更进一步的服务呢？组播（Multicast）和QoS正是正在或者已经加入到IP层协议中的两个特性。QoS单单通过终端系统，是无法实现的，与此不同的是，组播完全可以通过终端来实现。

那么组播到底放在哪里呢？首先回顾End-to-end Arguments（这是啥玩意儿？）的两个原则：
1、功能尽可能放到上层实现
2、除非下层在复杂性上做出牺牲可以换来巨大的性能提升，否则不在下层实现功能。

Deering1989认为，他出于以上第二个准则考虑后，认为组播应该放到IP层实现，而且很多人认为这是对的。但是实际上，IP组播，有很多弊端，首先路由必须维护每个组的状态（为什么啊？），这违背了“无状态”架构设计准则，而且增加了复杂性，降低了扩展性（how？）。IP层协议原本是最大努力单播协议，在其上实现高级功能非产困难（为啥？）。基于以上原因，IP组播没有大规模发展起来。

所以呢，Yang-hua Chu等哥们儿提出了终端系统组播（End System Multicast）。简单来说，就是把组播的实现上推到了终端，在IP层仅仅依赖单播（复杂了咋说呢？）。所谓的Narada Protocol就是实现该一思想的一个协议（终于进入正题了）。

其实，在Narada协议中，还没有明确提出过P2P的概念，但是为什么称其为P2P呢，我想原因大概是这样的，P2P顾名思义，就是平等的意思，大家都是一个Peer，功能上基本相似的（其实不完全一样，对于视频流协议来说，也不可能，因为必然有一个视频源，怎么可能和别的Peer对等呢，但是除了源与众不同以外，其它的节点应该都是平等的才对），那么终端组播的思想，其实就是将高级功能分配到每个终端，以此来实现组播，这么看来，每个终端就像是一个Peer一样，由此，Narada协议是一个P2P的协议。

昨天，一直看到很晚，虽然有一肚子的问题没有搞懂，但是还是让我挡不住地认为，这个协议蛮鹾的。首先，我来回忆一下Narada的拓扑结构，说白了这个Narada也是要构建覆盖图（Overlay，真是搞不懂这个东西）的，只不过是树状的覆盖图，其构建网络的过程是，先构造一个富连接图，称之为网（mesh），当然，这个图不是胡乱构建的，要满足一些性能上的指标才行，具体的，我想是视实际应用的网络而定的，比如根据网络延迟和节点距离来决定两个节点之间是否构造一条边等，接着，在这个网上，构造生成树集合，每棵生成树都以视频源为根（注意，该协议一开始就假设有多个视频源存在，对于单一的视频源也是可以适用的，但是其可靠性，就会下降）。

在Narada协议中，突然就提出了组（group）的概念，但是没有为其下明确的定义，我想，应该可以理解成上文提到的整个网的生成树吧。为了分散由单一节点管理整个组的信息的风险，组的信息管理和维护被分散到了每一个节点（P2P的思想，但是在这个协议中，真的不是分散啊，而是每个节点维护全部的组员的信息，所以说它鹾啊）。为了获知每个节点的活跃与否，组的每个成员（Member）被要求按照一定的周期向外发送一个唯一序列号，该序列号沿着整个网络传播（高冗余）。这样，每个组员至少要维护这么几个信息，每个组员的地址，组员i上回发来的串号k，接收到串号k的时间t。可以想见，如果一个节点发现某个节点超过一定的时间（Tm）没有发来串号，那么那个节点肯定有问题了，这个时候，一般启动恢复机制（实际上，由于网络的延迟，对于每一个节点来说，传播串号的时间是不一样的，所以，一个节点，相对于每个节点的超时时间Tm是不一样的，这也是应该维护的信息啊，但是文章中没有指出这一细节，我想，我的猜测还是合理的，因为串号沿着整个网络传播，如果一个节点真的失效，那么所有节点都会发觉有节点失效，按照协议，所有节点都启动恢复机制，对于一个死掉的节点，竟然有n-1个恢复连接请求在网上传播，真的是太鹾了）。如果恢复失败，就宣告死亡，每个组成员还要保留一个成员的死亡信息足够长时间。

上面说得比较顺，就先讲了这个节点脱离的处理机制，下面来讲讲，入伙的机制，这个其实比较简单的，说白了，就是傻等，首先获取一份组员列表（论文没有详细说怎么获取，向谁获取，不过，我想这个很简单的吧，因为每个组员都有完整的一个列表的），像列表中挑选出来的几个发送入伙请求（怎么挑呢？），然后傻等，加不断重试，直到入伙成功后，开始接受所有其他组员的信息并且自己也开始发送信息。

上面一系列的机制有个说法的，叫什么被动检测，主动测量（passive monitor，active mesurement）。我好像还没有看出active在什么地方，以后还需要好好咀嚼。

然后讲讲数据流的传输，流主要是沿着生成树来传播的，有个算法叫做反向最短路径算法，复述一下那个例子，如果M通过N接受S的源的话，那么当且仅当N是从M到S的必经的最近的路径时，N才向M转发数据。（这里还有一些东西要看的。感觉自己的语言好乏力，理论基础也太过薄弱了。）

Narada协议的适用范围是非常小的。这倒是不难理解，那么冗余的结构，每个节点保存全部其他节点的信息，可以想见，规模扩大后会发生什么，对此，此协议作者倒是相当诚实，他认为这个协议适用范围在几十到数百的组规模的应用。并给出了试验数据来说明。